Normal â‡ {
  ğ•Š randâ€¿âŸ¨mu, sigâŸ©:
    qâ€¿uâ€¿v â† 3â¥Š0
    {
      u â†© rand.Range 0
      v â†© 1.7156 Ã— (rand.Range 0) - 0.5
      x â† u - 0.449871
      y â† 0.386595 + |v
      q â†© (xâ‹†2) + y Ã— (yÃ—0.19600) - xÃ—0.25472
      ğ•ŠâŸ((q>0.27597) âˆ§ ((q>0.27846) âˆ¨ ((vâ‹†2) > Â¯4.0Ã—(â‹†â¼u)Ã—uâ‹†2))) @
    } @
    mu+sigÃ—vÃ·u
  ;
  ğ•Š rand:
    Normal randâ€¿âŸ¨0, 1âŸ©
  ;
  shape ğ•Š randâ€¿âŸ¨mu, sigâŸ©:
    { Normal (randâ€¿âŸ¨ğ•©, sigâŸ©) }Â¨ shapeâ¥Šmu
  ;
  shape ğ•Š rand:
    shape Normal randâ€¿âŸ¨0, 1âŸ©
}

InitializeParameters â‡ {
  nxâ€¿nhâ€¿ny:
  rand â† â€¢MakeRand 0
  parameters â† {
    w1 â‡ nhâ€¿nx Normal rand
    b1 â‡ nhâ¥Š0
    w2 â‡ nyâ€¿nh Normal rand
    b2 â‡ nyâ¥Š0
  }
}

InitializeParametersDeep â‡ {
  ğ•Š ns:  # output size of each layer: input, hiddens, output
  rand â† â€¢MakeRand 0
  ws â† (NormalâŸœrand)Â¨<Ë˜âŒ½Ë˜2â†•ns
  bs â† â¥ŠâŸœ0Â¨ 1â†“ns
  wsâ€¿bs
}

âŸ¨DotâŸ© â† â€¢Import "../../../libs/numb.bqn"

LinearForward â‡ {
  aâ€¿wâ€¿b:
  # a is mÃ—n, n is number of examples
  z â† (w Dot a) + b
  cache â† âŸ¨aâ€¿wâ€¿bâŸ©
  zâ€¿cache
}

Sigmoid â‡ {
  cache â† ğ•©
  a â† Ã·1+â‹†-ğ•©
  aâ€¿cache
}

ReLU â‡ {
  cache â† ğ•©
  a â† 0âŒˆğ•©
  aâ€¿cache
}

LinearActivationForward â‡ {
  aPrevâ€¿wâ€¿bâ€¿activation:
  # linearCache: a w b
  zâ€¿linearCache â† LinearForward aPrevâ€¿wâ€¿b
  # activationCache: z
  aâ€¿activationCache â† Activation z
  # cache: [a w b], z
  cache â† linearCacheâ€¿activationCache
  aâ€¿cache
}

LModelForward â‡ {
  xâ€¿wsâ€¿bs:
  a â† x
  cache â† @
  caches â† âŸ¨âŸ©
  {
    wâ€¿b:
    aâ€¿cache â†© LinearActivationForward aâ€¿wâ€¿bâ€¿reLU
    caches âˆ¾â†© â‹ˆcache
  }Â¨ Â¯1â†“ wsâ‹ˆÂ¨bs
  aâ€¿cache â†© LinearActivationForward aâ€¿(Â¯1âŠ‘ws)â€¿(Â¯1âŠ‘bs)â€¿sigmoid
  # caches: [ [ [a w b], z ], [ [a w b], z ], ... ]
  caches âˆ¾â†© â‹ˆcache
  aâ€¿caches
}

ComputeCost â‡ {
  alâ€¿y:
  m â† 1âŠ‘â‰¢y
  cost â† -mÃ·Ëœ+Â´â¥Š(yÃ—â‹†â¼al)+(Â¬y)Ã—â‹†â¼Â¬al
}

LinearBackward â‡ {
  # cache: linearCache: a w b
  dzâ€¿cache:
  aPrevâ€¿wâ€¿b â† cache
  m â† 1âŠ‘â‰¢aPrev
  dw â† mÃ·Ëœ dz Dot â‰aPrev
  db â† mÃ·Ëœ â¥Š+Â´Ë˜dz
  daPrev â† (â‰w) Dot dz
  daPrevâ€¿dwâ€¿db
}

SigmoidBackward â‡ {
  # cache: z
  daâ€¿cache:
  z â† cache
  s â† Ã·1+â‹†-z
  dz â† da Ã— s Ã— Â¬s
}

ReLUBackward â‡ {
  # cache: z
  daâ€¿cache:
  z â† cache
  â€¢Out "ReLUBackward"
  â€¢Show da
  â€¢Show z
  dz â† (z>0) Ã— da
}

LinearActivationBackward â‡ {
  daâ€¿cacheâ€¿activationBackward:
  linearCacheâ€¿activationCache â† cache
  dz â† ActivationBackward daâ€¿activationCache
  daPrevâ€¿dwâ€¿db â† LinearBackward dzâ€¿linearCache
  daPrevâ€¿dwâ€¿db
}

LModelBackward â‡ {
  aLâ€¿yâ€¿caches:
  grads â† âŸ¨âŸ©
  cache â† Â¯1âŠ‘caches
  â€¢Out "cache last"
  â€¢Show cache
  daL â† - (yÃ·aL) - (Â¬y)Ã·Â¬al
  daâ€¿dwâ€¿db â† LinearActivationBackward daLâ€¿cacheâ€¿SigmoidBackward
  grads âˆ¾â†© â‹ˆdaâ€¿dwâ€¿db
  {
    ğ•Š cache:
    daâ€¿dwâ€¿db â†© LinearActivationBackward daâ€¿cacheâ€¿ReLUBackward
    grads âˆ¾â†© â‹ˆdaâ€¿dwâ€¿db
  }Â¨ âŒ½Â¯1â†“caches
  âŒ½grads
}

UpdateParameters â‡ {
  parametersâ€¿gradsâ€¿learningRate:
  gradsNoA â† 1â†“Â¨ grads
  parameters -â†© learningRate Ã— gradsNoA
}
