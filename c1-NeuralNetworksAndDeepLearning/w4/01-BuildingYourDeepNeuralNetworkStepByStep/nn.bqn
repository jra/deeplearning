Normal â‡ {
  ğ•Š randâ€¿âŸ¨mu, sigâŸ©:
    qâ€¿uâ€¿v â† 3â¥Š0
    {
      u â†© rand.Range 0
      v â†© 1.7156 Ã— (rand.Range 0) - 0.5
      x â† u - 0.449871
      y â† 0.386595 + |v
      q â†© (xâ‹†2) + y Ã— (yÃ—0.19600) - xÃ—0.25472
      ğ•ŠâŸ((q>0.27597) âˆ§ ((q>0.27846) âˆ¨ ((vâ‹†2) > Â¯4.0Ã—(â‹†â¼u)Ã—uâ‹†2))) @
    } @
    mu+sigÃ—vÃ·u
  ;
  ğ•Š rand:
    Normal randâ€¿âŸ¨0, 1âŸ©
  ;
  shape ğ•Š randâ€¿âŸ¨mu, sigâŸ©:
    { Normal (randâ€¿âŸ¨ğ•©, sigâŸ©) }Â¨ shapeâ¥Šmu
  ;
  shape ğ•Š rand:
    shape Normal randâ€¿âŸ¨0, 1âŸ©
}

InitializeParameters â‡ {
  nxâ€¿nhâ€¿ny:
  rand â† â€¢MakeRand 0
  w1 â† 0.01 Ã— nhâ€¿nx Normal rand
  b1 â† nhâ¥Š0
  w2 â† 0.01 Ã— nyâ€¿nh Normal rand
  b2 â† nyâ¥Š0
  parameters â† âŸ¨w1â€¿b1, w2â€¿b2âŸ©
}

InitializeParametersDeep â‡ {
  ğ•Š ns:  # output size of each layer: input, hiddens, output
  rand â† â€¢MakeRand 0
  #ws â† 0.01 Ã— (NormalâŸœrand)Â¨<Ë˜âŒ½Ë˜2â†•ns
  ws â† (NormalâŸœrandÂ¨<Ë˜âŒ½Ë˜2â†•ns) Ã· âˆšÂ¯1â†“ns
  bs â† â¥ŠâŸœ0Â¨ 1â†“ns
  wsâ‹ˆÂ¨bs
}

âŸ¨DotâŸ© â† â€¢Import "../../../libs/numb.bqn"
#Dot â† {
#  1â‰¡âŠ‘â‰ ğ•¨? â‰+Â´ (â¥Šğ•¨) Ã— <Ë˜ ğ•©;
#  1â‰¡1âŠ‘â‰¢ğ•©? (â‰ ğ•¨)â€¿1â¥Š (+Â´(â¥Šğ•©)âŠ¸Ã—)Ë˜ğ•¨;
#  ğ•¨ +Ëâˆ˜Ã—â‰1â€¿âˆ ğ•©
#}

LinearForward â‡ {
  aâ€¿wâ€¿b:
  # a is mÃ—n, n is number of examples
  z â† (w Dot a) + b
  cache â† aâ€¿wâ€¿b
  zâ€¿cache
}

Sigmoid â‡ {
  cache â† ğ•©
  a â† Ã·1+â‹†-ğ•©
  aâ€¿cache
}

ReLU â‡ {
  cache â† ğ•©
  a â† 0âŒˆğ•©
  aâ€¿cache
}

LinearActivationForward â‡ {
  aPrevâ€¿wâ€¿bâ€¿activation:
  # linearCache: a w b
  zâ€¿linearCache â† LinearForward aPrevâ€¿wâ€¿b
  # activationCache: z
  aâ€¿activationCache â† Activation z
  # cache: [a w b], z
  cache â† linearCacheâ€¿activationCache
  aâ€¿cache
}

LModelForward â‡ {
  xâ€¿ps:
  a â† x
  cache â† @
  caches â† {
    wâ€¿b:
    aâ€¿cache â†© LinearActivationForward aâ€¿wâ€¿bâ€¿reLU
    cache
  }Â¨ Â¯1â†“ps
  wâ€¿b â† Â¯1âŠ‘ps
  aâ€¿cache â†© LinearActivationForward aâ€¿wâ€¿bâ€¿sigmoid
  # caches: [ [ [a w b], z ], [ [a w b], z ], ... ]
  caches âˆ¾â†© â‹ˆcache
  aâ€¿caches
}

ComputeCost â‡ {
  alâ€¿y:
  m â† 1âŠ‘â‰¢y
  cost â† -mÃ·Ëœ+Â´â¥Š(yÃ—â‹†â¼al)+(Â¬y)Ã—â‹†â¼Â¬al
}

LinearBackward â‡ {
  # cache: linearCache: a w b
  dzâ€¿cache:
  aPrevâ€¿wâ€¿b â† cache
  m â† 1âŠ‘â‰¢aPrev
  dw â† mÃ·Ëœ dz Dot â‰aPrev
  db â† mÃ·Ëœ â¥Š+Â´Ë˜dz
  daPrev â† (â‰w) Dot dz
  daPrevâ€¿dwâ€¿db
}

SigmoidBackward â‡ {
  # cache: z
  daâ€¿cache:
  z â† cache
  s â† Ã·1+â‹†-z
  dz â† da Ã— s Ã— Â¬s
}

ReLUBackward â‡ {
  # cache: z
  daâ€¿cache:
  z â† cache
  dz â† (z>0) Ã— da
}

LinearActivationBackward â‡ {
  daâ€¿cacheâ€¿activationBackward:
  linearCacheâ€¿activationCache â† cache
  dz â† ActivationBackward daâ€¿activationCache
  daPrevâ€¿dwâ€¿db â† LinearBackward dzâ€¿linearCache
  daPrevâ€¿dwâ€¿db
}

LModelBackward â‡ {
  aLâ€¿yâ€¿caches:
  cache â† Â¯1âŠ‘caches
  daL â† - (yÃ·aL) - (Â¬y)Ã·Â¬aL
  daâ€¿dwâ€¿db â† LinearActivationBackward daLâ€¿cacheâ€¿SigmoidBackward
  grads â† âŸ¨daâ€¿dwâ€¿dbâŸ©
  grads âˆ¾â†© {
    ğ•Š cache:
    daâ€¿dwâ€¿db â†© LinearActivationBackward daâ€¿cacheâ€¿ReLUBackward
    daâ€¿dwâ€¿db
  }Â¨ âŒ½Â¯1â†“caches
  âŒ½grads
}

UpdateParameters â‡ {
  parametersâ€¿gradsâ€¿learningRate:
  gradsNoA â† 1â†“Â¨ grads
  parameters -â†© learningRate Ã— gradsNoA
}
