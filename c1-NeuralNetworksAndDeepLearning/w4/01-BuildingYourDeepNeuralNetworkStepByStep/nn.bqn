Normal â‡ {
  ğ•Š randâ€¿âŸ¨mu, sigâŸ©:
    qâ€¿uâ€¿v â† 3â¥Š0
    {
      u â†© rand.Range 0
      v â†© 1.7156 Ã— (rand.Range 0) - 0.5
      x â† u - 0.449871
      y â† 0.386595 + |v
      q â†© (xâ‹†2) + y Ã— (yÃ—0.19600) - xÃ—0.25472
      ğ•ŠâŸ((q>0.27597) âˆ§ ((q>0.27846) âˆ¨ ((vâ‹†2) > Â¯4.0Ã—(â‹†â¼u)Ã—uâ‹†2))) @
    } @
    mu+sigÃ—vÃ·u
  ;
  ğ•Š rand:
    Normal randâ€¿âŸ¨0, 1âŸ©
  ;
  shape ğ•Š randâ€¿âŸ¨mu, sigâŸ©:
    { Normal (randâ€¿âŸ¨ğ•©, sigâŸ©) }Â¨ shapeâ¥Šmu
  ;
  shape ğ•Š rand:
    shape Normal randâ€¿âŸ¨0, 1âŸ©
}

InitializeParameters â‡ {
  nxâ€¿nhâ€¿ny:
  rand â† â€¢MakeRand 0
  parameters â† {
    w1 â‡ nhâ€¿nx Normal rand
    b1 â‡ nhâ¥Š0
    w2 â‡ nyâ€¿nh Normal rand
    b2 â‡ nyâ¥Š0
  }
}

InitializeParametersDeep â‡ {
  ğ•Š ns:  # output size of each layer: input, hiddens, output
  rand â† â€¢MakeRand 0
  ws â† (NormalâŸœrand)Â¨<Ë˜âŒ½Ë˜2â†•ns
  bs â† â¥ŠâŸœ0Â¨ 1â†“ns
  wsâ€¿bs
}

âŸ¨DotâŸ© â† â€¢Import "../../../libs/numb.bqn"

LinearForward â‡ {
  aâ€¿wâ€¿b:
  # a is mÃ—n, n is number of examples
  z â† (w Dot a) + b
  zâ€¿âŸ¨aâ€¿wâ€¿bâŸ©
}

Sigmoid â‡ {
  cache â† ğ•©
  a â† Ã·1+â‹†-ğ•©
  aâ€¿cache
}

ReLU â‡ {
  cache â† ğ•©
  a â† 0âŒˆğ•©
  aâ€¿cache
}

LinearActivationForward â‡ {
  aPrevâ€¿wâ€¿bâ€¿activation:
  zâ€¿linearCache â† LinearForward aPrevâ€¿wâ€¿b
  aâ€¿activationCache â† Activation z
  cache â† linearCacheâ€¿activationCache
  aâ€¿cache
}

LModelForward â‡ {
  xâ€¿wsâ€¿bs:
  a â† x
  cache â† @
  caches â† âŸ¨âŸ©
  {
    wâ€¿b:
    aâ€¿cache â†© LinearActivationForward aâ€¿wâ€¿bâ€¿reLU
    caches âˆ¾â†© â‹ˆcache
  }Â¨ Â¯1â†“ wsâ‹ˆÂ¨bs
  aâ€¿cache â†© LinearActivationForward aâ€¿(Â¯1âŠ‘ws)â€¿(Â¯1âŠ‘bs)â€¿sigmoid
  caches âˆ¾â†© â‹ˆcache
  aâ€¿caches
}

ComputeCost â‡ {
  alâ€¿y:
  m â† 1âŠ‘â‰¢y
  cost â† -mÃ·Ëœ+Â´â¥Š(yÃ—â‹†â¼al)+(Â¬y)Ã—â‹†â¼Â¬al
}
