path_libreaddata â† "/Users/jra/prj/coursera/deeplearning/c1-NeuralNetworksAndDeepLearning/w2/02-LogisticRegressionWithANeuralNetworkMindset/libreaddata.dylib"
readTrainX â† path_libreaddata â€¢FFI "a"â€¿"read_train_x"
readTrainY â† path_libreaddata â€¢FFI "a"â€¿"read_train_y"
readTestX â† path_libreaddata â€¢FFI "a"â€¿"read_test_x"
readTestY â† path_libreaddata â€¢FFI "a"â€¿"read_test_y"
readTestListClasses â† path_libreaddata â€¢FFI "a"â€¿"read_test_list_classes"

train_x_orig â† @ -Ëœ ReadTrainX âŸ¨âŸ©
train_y â† 1â€¿âˆ˜â¥Š ReadTrainY âŸ¨âŸ©
test_x_orig â† @ -Ëœ ReadTestX âŸ¨âŸ©
test_y â† 1â€¿âˆ˜â¥Š ReadTestY âŸ¨âŸ©
classes â† ReadTestListClasses âŸ¨âŸ©

index â† 25
#index â† 208
â€¢Out "y = "âˆ¾(â€¢Repr indexâŠ‘âŠtrain_y)âˆ¾", it's a '"âˆ¾((indexâŠ‘âŠtrain_y)âŠ‘classes)âˆ¾"' picture."
ImageWrite â† {
  # write image to file:
  # ppm ex.
  # 4 8  4 columns, 8 rows
  # 255  max value
  # data read in rows
  rowsâ€¿colsâ€¿ignore â† â‰¢ğ•©  # ie 64 Ã— 64 Ã— 3
  header_ppm â† "P6
"âˆ¾(â€¢Repr cols)âˆ¾" "âˆ¾(â€¢Repr rows)âˆ¾"
255
"
  image_ppm â† @âŠ¸+Â¨ â¥Šğ•©
  bytes_ppm â† header_ppm âˆ¾ image_ppm
  ğ•¨ â€¢file.Bytes bytes_ppm
}
("img-train"âˆ¾(â€¢Repr index)âˆ¾".ppm") ImageWrite index âŠ train_x_orig

m_train â† â‰ train_x_orig
m_test â† â‰ test_x_orig
num_px â† 1âŠ‘â‰¢train_x_orig

â€¢Out "Number of training examples: m_train = "âˆ¾â€¢Repr m_train
â€¢Out "Number of testing examples: m_test = "âˆ¾â€¢Repr m_test
â€¢Out "Height/Width of each image: num_px = "âˆ¾â€¢Repr num_px
â€¢Out "Each image is of size: "âˆ¾â€¢Repr â‰¢âŠtrain_x_orig
â€¢Out "train_x shape: "âˆ¾â€¢Repr â‰¢train_x_orig
â€¢Out "train_y shape: "âˆ¾â€¢Repr â‰¢train_y
â€¢Out "test_x shape: "âˆ¾â€¢Repr â‰¢test_x_orig
â€¢Out "test_y shape: "âˆ¾â€¢Repr â‰¢test_y

train_x_flatten â† â‰ (â‰ train_x_orig)â€¿âˆ˜ â¥Š train_x_orig
test_x_flatten â† â‰ (â‰ test_x_orig)â€¿âˆ˜ â¥Š test_x_orig

train_x â† train_x_flatten Ã· 255
test_x â† test_x_flatten Ã· 255

ReportArray â† {
  nmâ€¿ar:
  n â† â‰ â¥Šar
  mean â† (+Â´Ã·â‰ )â¥Šar
  {ğ•¤
    â€¢Show mean
    â€¢Show mean - ar
    â€¢Show â‹†âŸœ2 mean - ar
  }âŸ(n<0) @
  stdv â† âˆš Ã·âŸœn +Â´ â¥Š â‹†âŸœ2 mean - ar
  â€¢Out nmâˆ¾": shape: "âˆ¾(â€¢Repr â‰¢ar)âˆ¾" mean: "âˆ¾(â€¢Repr mean)âˆ¾" stdv: "âˆ¾(â€¢Repr stdv)
}

ReportArray "train_x"â€¿train_x
ReportArray "train_y"â€¿train_y

n_x â† âŠ‘â‰¢train_x  # 12288 num_px * num_px * 3
n_h â† 7
n_y â† 1
layers_dims â† n_xâ€¿n_hâ€¿n_y

nn â† â€¢Import "../01-BuildingYourDeepNeuralNetworkStepByStep/nn.bqn"

TwoLayerModel â† {
  xâ€¿yâ€¿layers_dimsâ€¿learning_rateâ€¿num_iterationsâ€¿print_cost:
  costs â† @
  m â† 1âŠ‘â‰¢x
  #parameters â† nn.InitializeParameters layers_dims
  parameters â† nn.InitializeParametersDeep layers_dims
  #w1i â† 0âŠ‘0âŠ‘parameters
  #w2i â† 0âŠ‘1âŠ‘parameters
  #ReportArray "w1i"â€¿w1i
  #ReportArray "w2i"â€¿w2i
  i â† 0
  {ğ•¤
    âŸ¨w1â€¿b1, w2â€¿b2âŸ© â† parameters
    a1â€¿cache1 â† nn.LinearActivationForward xâ€¿w1â€¿b1â€¿nn.reLU
    a2â€¿cache2 â† nn.LinearActivationForward a1â€¿w2â€¿b2â€¿nn.sigmoid
    cost â† nn.ComputeCost a2â€¿y
    da2 â† - (yÃ·a2) - (Â¬y)Ã·Â¬a2
    da1â€¿dw2â€¿db2 â† nn.LinearActivationBackward da2â€¿cache2â€¿nn.sigmoidBackward
    da0â€¿dw1â€¿db1 â† nn.LinearActivationBackward da1â€¿cache1â€¿nn.reluBackward
    grads â† âŸ¨âŸ¨âŸ©â€¿dw1â€¿db1, âŸ¨âŸ©â€¿dw2â€¿db2âŸ©
    parameters â†© nn.UpdateParameters parametersâ€¿gradsâ€¿learning_rate
    {ğ•¤
      costs âˆ¾â†© cost
      {ğ•¤, â€¢Out "Cost after iteration "âˆ¾(â€¢Repr i)âˆ¾": "âˆ¾(â€¢Repr cost) }âŸprint_cost @
    }âŸ(0=100|i) @
    i +â†© 1
  }âŸnum_iterations @
  parameters
}

#parameters â† TwoLayerModel train_xâ€¿train_yâ€¿layers_dimsâ€¿0.0075â€¿1â€¿1
parameters â† TwoLayerModel train_xâ€¿train_yâ€¿layers_dimsâ€¿0.0075â€¿2500â€¿1
