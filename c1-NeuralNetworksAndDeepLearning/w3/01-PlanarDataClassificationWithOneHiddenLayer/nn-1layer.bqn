# Normal Deviates by Ratio-of-Uniforms
# Numerical Recipes 7.3.9
Normal â‡ {
  r ð•Š muâ€¿sig:
  rand â† â€¢MakeRand r
  qâ€¿uâ€¿v â† 3â¥Š0
  {
    u â†© rand.Range 0
    v â†© 1.7156 Ã— (rand.Range 0) - 0.5
    x â† u - 0.449871
    y â† 0.386595 + |v
    q â†© (xâ‹†2) + y Ã— (yÃ—0.19600) - xÃ—0.25472
    ð•ŠâŸ((q>0.27597) âˆ§ ((q>0.27846) âˆ¨ ((vâ‹†2) > Â¯4.0Ã—(â‹†â¼u)Ã—uâ‹†2))) @
  } @
  mu+sigÃ—vÃ·u
}

#rng â† â€¢MakeRand 0

#n â† rng Normal 0â€¿1
#â€¢Show n
#n â†© rng Normal 0â€¿1
#â€¢Show n
#ns â† {rng Normal 0â€¿1}Â¨100â¥Š0
#â€¢Show ns

#n â† (rng.Range 0) Normal 0â€¿1
#â€¢Show n
#n â†© (rng.Range 0) Normal 0â€¿1
#â€¢Show n
#ns â† {ð•© Normal 0â€¿1}Â¨ 1000 rng.Range 0
#â€¢Show â‰ ns
#â€¢Show âŒŠÂ´ns
#â€¢Show âŒˆÂ´ns
#mean_ns â† (+Â´Ã·â‰ ) ns
#â€¢Out "mean_ns: "âˆ¾â€¢Repr mean_ns
#var_ns â† (â‰ ns)Ã·Ëœ+Â´2â‹†Ëœns-mean_ns
#â€¢Out "var_ns: "âˆ¾â€¢Repr var_ns

FmtNeg â‡ {
  a â† â€¢Fmt ð•©
  (âŠ£+=Ã—'-'âŠ¸-)âŸœ'Â¯' a
}

#"normals.csv" â€¢file.Lines âŸ¨"vals"âŸ©âˆ¾FmtNegÂ¨ns

LayerSizes â‡ {
  xâ€¿y:
  n_x â† âŠ‘â‰¢x
  n_h â† 4
  n_y â† (1==y)âŠ‘âŸ¨âŠ‘â‰¢y, 1âŸ©
  n_xâ€¿n_hâ€¿n_y
}

InitializeParameters â‡ {
  n_xâ€¿n_hâ€¿n_y:
  rng â† â€¢MakeRand 2
  cache â† {
    w1 â‡ 0.01 Ã— {ð•© Normal 0â€¿1}Â¨ n_hâ€¿n_x rng.Range 0
    b1 â‡ n_hâ€¿1â¥Š0
    w2 â‡ 0.01 Ã— {ð•© Normal 0â€¿1}Â¨ n_yâ€¿n_h rng.Range 0
    b2 â‡ n_yâ€¿1â¥Š0
  }
}

Sigmoid â† { Ã·1+â‹†-ð•© }
TanH â† { (1-Ëœâ‹†2Ã—ð•©) Ã· 1+â‹†2Ã—ð•© }
#TanH â† { â€¢math.tanh }
#ReLU â† { ð•©âŒˆ0 }
#ReLULeaky â† { ð•©âŒˆ0.01Ã—ð•© }

âŸ¨DotâŸ© â† â€¢Import "../../../libs/numb.bqn"

ForwardPropagation â‡ {
  xâ€¿âŸ¨w1, b1, w2, b2âŸ©:
  {
    # I'd use âŠË˜ or â‰â¼Ë˜ to remove an extra trailing axis from a column vector.
    # Note that +Ëâˆ˜Ã—âŽ‰1â€¿âˆž doesn't require any special casing for lists. It treats lists as row vectors on the left, or column vectors on the right.
    # I think (m +Ëâˆ˜Ã—âŽ‰1â€¿âˆž x) + b should work whether x is a list or nÃ—1 column, if b is a list.
    #  treat vectors as lists and rearrange my thinking/equations
    # Yeah. I tend to think it's the matrix product that's confusing. What it does is to pair the last axis of ð•¨ with the first axis of ð•©. That shared axis gets reduced out, and any other axes (none in the case of a list) go into the result, in order.
    z1 â‡ b1 â¥ŠâŠ¸+ w1 Dot x  # TODO better way to broadcast?
    a1 â‡ TanhÂ¨ z1
    z2 â‡ b2 â¥ŠâŠ¸+ w2 Dot a1
    a2 â‡ SigmoidÂ¨ z2
  }
}

ComputeCost â‡ {
  a2â€¿yâ€¿âŸ¨w1, b1, w2, b2âŸ©:
  m â† 1âŠ‘â‰¢y
  cost â† -mÃ·Ëœ+Â´â¥Š(yÃ—â‹†â¼a2)+(Â¬y)Ã—â‹†â¼Â¬a2
}

BackwardPropagation â‡ {
  âŸ¨w1,b1,w2,b2âŸ©â€¿âŸ¨z1,a1,z2,a2âŸ©â€¿xâ€¿y:
  m â† 1âŠ‘â‰¢y
  dz2 â† a2-y
  dw2 â† mÃ·Ëœ dz2 Dot â‰a1
  db2 â† mÃ·Ëœ â‰ +Â´Ë˜ dz2
  dz1 â† ((â‰w2) Dot dz2) Ã— Â¬a1â‹†2
  dw1 â† mÃ·Ëœ dz1 Dot â‰x
  db1 â† mÃ·Ëœ â‰ â‰ +Â´Ë˜ dz1
  grads â† {
    dw1 â‡ dw1
    db1 â‡ db1
    dw2 â‡ dw2
    db2 â‡ db2
  }
}

UpdateParameters â‡ {
  âŸ¨w1,b1,w2,b2âŸ©â€¿âŸ¨dw1,db1,dw2,db2âŸ©â€¿learning_rate:
  parameters â† {
    w1 â‡ w1 - learning_rate Ã— dw1
    b1 â‡ b1 - learning_rate Ã— db1
    w2 â‡ w2 - learning_rate Ã— dw2
    b2 â‡ b2 - learning_rate Ã— db2
  }
}

NNModel â‡ {
  xâ€¿yâ€¿n_hâ€¿learning_rateâ€¿num_iterationsâ€¿print_cost:
  n_xâ€¿Â·â€¿n_y â† LayerSizes xâ€¿y
  cacheâ€¿costâ€¿grads â† @â€¿@â€¿@
  parameters â† InitializeParameters n_xâ€¿n_hâ€¿n_y
  i â† 0
  {ð•¤
    cache â†© ForwardPropagation xâ€¿parameters
    cost â†© ComputeCost cache.a2â€¿yâ€¿parameters
    grads â†© BackwardPropagation parametersâ€¿cacheâ€¿xâ€¿y
    parameters â†© UpdateParameters parametersâ€¿gradsâ€¿learning_rate
    {ð•¤
      â€¢Out "Costs after iteration "âˆ¾(â€¢Repr i)âˆ¾": "âˆ¾â€¢Repr cost
    }âŸ(print_cost âˆ§ 0=1000|i) @
    i +â†© 1
  }âŸnum_iterations @
  parameters
}

Predict â‡ {
  parametersâ€¿x:
  âŸ¨a2âŸ© â† ForwardPropagation xâ€¿parameters
  predictions â† a2 > 0.5
}
