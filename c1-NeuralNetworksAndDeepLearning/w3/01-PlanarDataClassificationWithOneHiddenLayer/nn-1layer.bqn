# Normal Deviates by Ratio-of-Uniforms
# Numerical Recipes 7.3.9
Normal â† {
  r ğ•Š muâ€¿sig:
  rand â† â€¢MakeRand r
  qâ€¿uâ€¿v â† 3â¥Š0
  {
    u â†© rand.Range 0
    v â†© 1.7156 Ã— (rand.Range 0) - 0.5
    x â† u - 0.449871
    y â† 0.386595 + |v
    q â†© (xâ‹†2) + y Ã— (yÃ—0.19600) - xÃ—0.25472
    ğ•ŠâŸ((q>0.27597) âˆ§ ((q>0.27846) âˆ¨ ((vâ‹†2) > Â¯4.0Ã—(â‹†â¼u)Ã—uâ‹†2))) @
  } @
  mu+sigÃ—vÃ·u
}

rng â† â€¢MakeRand 0

#n â† rng Normal 0â€¿1
#â€¢Show n
#n â†© rng Normal 0â€¿1
#â€¢Show n
#ns â† {rng Normal 0â€¿1}Â¨100â¥Š0
#â€¢Show ns

#n â† (rng.Range 0) Normal 0â€¿1
#â€¢Show n
#n â†© (rng.Range 0) Normal 0â€¿1
#â€¢Show n
#ns â† {ğ•© Normal 0â€¿1}Â¨ 1000 rng.Range 0
#â€¢Show â‰ ns
#â€¢Show âŒŠÂ´ns
#â€¢Show âŒˆÂ´ns
#mean_ns â† (+Â´Ã·â‰ ) ns
#â€¢Out "mean_ns: "âˆ¾â€¢Repr mean_ns
#var_ns â† (â‰ ns)Ã·Ëœ+Â´2â‹†Ëœns-mean_ns
#â€¢Out "var_ns: "âˆ¾â€¢Repr var_ns

FmtNeg â† {
  a â† â€¢Fmt ğ•©
  (âŠ£+=Ã—'-'âŠ¸-)âŸœ'Â¯' a
}
#
#"normals.csv" â€¢file.Lines âŸ¨"vals"âŸ©âˆ¾FmtNegÂ¨ns

LoadPlanarDataset â† {
  Â·:
  rng â† â€¢MakeRand 1
  m â† 400 # number of examples
  n â† âŒŠmÃ·2 # number of points per class
  d â† 2 # dimensionality
  #x â† mâ€¿Dâ¥Š0 # data matrix where each row is a single example
  #y â† mâ€¿1â¥Š0 # labels vector (0 for red, 1 for blue)
  a â† 4 # maximum ray of the flower
  xy â† {
    t â† ((3.12Ã—ğ•©) + (3.12Ã—(n-1)Ã·Ëœâ†•n)) + 0.2 Ã— {ğ•© Normal 0â€¿1}Â¨ n rng.Range 0
    r â† (aÃ—â€¢math.SinÂ¨4Ã—t) + 0.2 Ã— {ğ•© Normal 0â€¿1}Â¨ n rng.Range 0
    x0 â† rÃ—â€¢math.SinÂ¨t
    x1 â† rÃ—â€¢math.CosÂ¨t
    y â† nâ¥Šğ•©
    (â‰x0â‰x1)â€¿y
  }Â¨0â€¿1
  x â† âˆ¾âŠ‘Â¨xy
  y â† â‰â‰âˆ¾1âŠ¸âŠ‘Â¨xy
  â‰Â¨xâ€¿y
}

xâ€¿y â† LoadPlanarDataset @
#â€¢Show â‰¢âŠ‘xy
#â€¢Show â‰¢1âŠ‘xy

#ds â† â‰(âŠ‘xy)âˆ¾1âŠ‘xy
ds â† (â‰x)âˆ¾Ë˜â‰y
â€¢Out "ds:"
â€¢Show â‰¢ds
"dataset.csv" â€¢file.Lines âŸ¨"x0,x1,y"âŸ©âˆ¾{(FmtNeg 0âŠ‘ğ•©)âˆ¾","âˆ¾(FmtNeg 1âŠ‘ğ•©)âˆ¾","âˆ¾(FmtNeg 2âŠ‘ğ•©)}Â¨<Ë˜ds

#x â† âŠ‘xy
#y â† 1âŠ‘xy
shape_x â† â‰¢x
shape_y â† â‰¢y
m â† â‰ y
â€¢Out "The shape of x is: "âˆ¾â€¢Repr shape_x
â€¢Out "The shape of y is: "âˆ¾â€¢Repr shape_y
â€¢Out "I have m = "âˆ¾(â€¢Repr m)âˆ¾" training examples!"

LayerSizes â‡ {
  xâ€¿y:
  n_x â† âŠ‘â‰¢x
  n_h â† 4
  n_y â† (1==y)âŠ‘âŸ¨âŠ‘â‰¢y, 1âŸ©
  n_xâ€¿n_hâ€¿n_y
}

n_xâ€¿n_hâ€¿n_y â† 3â¥Š@
x_assessâ€¿y_assess â† 2â¥Š@

InitializeParameters â† {
  n_xâ€¿n_hâ€¿n_y:
  rng â† â€¢MakeRand 2

  w1 â† 0.01 Ã— {ğ•© Normal 0â€¿1}Â¨ n_hâ€¿n_x rng.Range 0
  b1 â† n_hâ€¿1â¥Š0
  w2 â† 0.01 Ã— {ğ•© Normal 0â€¿1}Â¨ n_yâ€¿n_h rng.Range 0
  b2 â† n_yâ€¿1â¥Š0

  â€¢Show â‰¢w1
  â€¢Show â‰¢b1
  â€¢Show â‰¢w2
  â€¢Show â‰¢b2

  #! n_hâ€¿n_x=â‰¢w1
  #! n_hâ€¿1=â‰¢b1
  #! n_yâ€¿n_h=â‰¢w2
  #! n_yâ€¿1=â‰¢b2

  {
    w1â‡w1
    b1â‡b1
    w2â‡w2
    b2â‡b2
  }
}

InitializeParametersTestCase â† {
  Â·:
  n_xâ€¿n_hâ€¿n_y â† 2â€¿4â€¿1
  n_xâ€¿n_hâ€¿n_y
}

n_xâ€¿n_hâ€¿n_y â†© InitializeParametersTestCase @

parameters â† InitializeParameters n_xâ€¿n_hâ€¿n_y
#â€¢Out "W1 = "âˆ¾â€¢Repr 0âŠ‘parameters
#â€¢Out "b1 = "âˆ¾â€¢Repr 1âŠ‘parameters
#â€¢Out "W2 = "âˆ¾â€¢Repr 2âŠ‘parameters
#â€¢Out "b2 = "âˆ¾â€¢Repr 3âŠ‘parameters

â€¢Out "W1 = "âˆ¾â€¢Repr parameters.w1
â€¢Out "b1 = "âˆ¾â€¢Repr parameters.b1
â€¢Out "W2 = "âˆ¾â€¢Repr parameters.w2
â€¢Out "b2 = "âˆ¾â€¢Repr parameters.b2

Sigmoid â† { Ã·1+â‹†-ğ•© }
TanH â† { (1-Ëœâ‹†2Ã—ğ•©) Ã· 1+â‹†2Ã—ğ•© }
#TanH â† { â€¢math.tanh }
#ReLU â† { ğ•©âŒˆ0 }
#ReLULeaky â† { ğ•©âŒˆ0.01Ã—ğ•© }

âŸ¨DotâŸ© â† â€¢Import "../../w2/02-LogisticRegressionWithANeuralNetworkMindset/numb.bqn"
ForwardPropagation â† {
  xâ€¿âŸ¨w1, b1, w2, b2âŸ©:
  {
    #z1â‡(â¥Šb1) +â‰0â€¿1 w1 Dot x  # TODO better way to broadcast?
    z1â‡(â¥Šb1) + w1 Dot x  # TODO better way to broadcast?
    a1â‡TanhÂ¨ z1
    z2â‡(â¥Šb2) + w2 Dot a1
    a2â‡SigmoidÂ¨ z2
  }
}

ForwardPropagationTestCase â† {
  Â·:
  #rng â† â€¢MakeRand 1
  #x_assess â† {ğ•© Normal 0â€¿1}Â¨ 2â€¿3 rng.Range 0
  x_assess â† [[ 1.62434536, -0.61175641, -0.52817175]
              [-1.07296862,  0.86540763, -2.3015387 ]]
  #b1 â† {ğ•© Normal 0â€¿1}Â¨ 4â€¿1 rng.Range 0
  b1 â† [[ 1.74481176]
        [-0.7612069 ]
        [ 0.3190391 ]
        [-0.24937038]]
  b2 â† 1â€¿1â¥ŠÂ¯1.3
  parameters â† {
    w1â‡[[-0.00416758, -0.00056267]
        [-0.02136196,  0.01640271]
        [-0.01793436, -0.00841747]
        [ 0.00502881, -0.01245288]]
    b1â‡b1
    w2â‡[[-0.01057952, -0.00909008, 0.00551454, 0.02292208]]
    b2â‡b2
  }
  x_assessâ€¿parameters
}

x_assessâ€¿parameters â†© ForwardPropagationTestCase @

â€¢Show x_assess
â€¢Out "w1"
â€¢Show parameters.w1
â€¢Out "b1"
â€¢Show parameters.b1
â€¢Out "w2"
â€¢Show parameters.w2
â€¢Out "b2"
â€¢Show parameters.b2

cache â† ForwardPropagation x_assessâ€¿parameters
Mean â† { (+Â´â¥Šğ•©) Ã· (â‰ â¥Šğ•©) }  # TODO better way?
â€¢Show (Mean cache.z1)â€¿(Mean cache.a1)â€¿(Mean cache.z2)â€¿(Mean cache.a2)

ComputeCost â† {
  a2â€¿yâ€¿âŸ¨w1, b1, w2, b2âŸ©:
  m â† 1âŠ‘â‰¢y
  -mÃ·Ëœ+Â´â¥Š(yÃ—â‹†â¼a2)+(Â¬y)Ã—â‹†â¼Â¬a2
}

ComputeCostTestCase â† {
  Â·:
  a2 â† [[0.5002307,  0.49985831, 0.50023963]]
  y_assess â† [[1, 0, 0]]
  parameters â† {
    w1 â‡ [[-0.00416758, -0.00056267]
          [-0.02136196,  0.01640271]
          [-0.01793436, -0.00841747]
          [ 0.00502881, -0.01245288]]
    w2 â‡ [[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]
    b1 â‡ [[0]
          [0]
          [0]
          [0]]
    b2 â‡ [[0]]
  }
  a2â€¿y_assessâ€¿parameters
}

a2 â† @
a2â€¿y_assessâ€¿parameters â†© ComputeCostTestCase @
cost â† ComputeCost a2â€¿y_assessâ€¿parameters
â€¢Show cost

BackwardPropagation â† {
  âŸ¨w1,b1,w2,b2âŸ©â€¿âŸ¨z1,a1,z2,a2âŸ©â€¿xâ€¿y:
  m â† 1âŠ‘â‰¢y
  dz2 â† a2-y
  dw2 â† mÃ·Ëœ dz2 Dot â‰a1
  #db2 â† mÃ·Ëœ +Â´ â¥Šdz2
  #â€¢Out "dz2:"
  #â€¢Show dz2
  #â€¢Show +Ë˜dz2
  db2 â† mÃ·Ëœ â‰ +Â´Ë˜ dz2
  dz1 â† ((â‰w2) Dot dz2) Ã— Â¬a1â‹†2
  #â€¢Out "dz1:"âˆ¾â€¢Repr â‰¢dz1
  dw1 â† mÃ·Ëœ dz1 Dot â‰x
  db1 â† mÃ·Ëœ â‰ â‰ +Â´Ë˜ dz1
  {
    dw1 â‡ dw1
    db1 â‡ db1
    dw2 â‡ dw2
    db2 â‡ db2
  }
}

BackwardPropagationTestCase â† {
  Â·:
  parameters â† {
    w1 â‡ [[-0.00416758, -0.00056267]
          [-0.02136196,  0.01640271]
          [-0.01793436, -0.00841747]
          [ 0.00502881, -0.01245288]]
    w2 â‡ [[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]
    b1 â‡ [[0]
          [0]
          [0]
          [0]]
    b2 â‡ [[0]]
  }
  cache â† {
    a1 â‡ [[-0.00616578,  0.0020626 ,  0.00349619]
          [-0.05225116,  0.02725659, -0.02646251]
          [-0.02009721,  0.0036869 ,  0.02883756]
          [ 0.02152675, -0.01385234,  0.02599885]]
    a2 â‡ [[0.5002307 , 0.49985831, 0.50023963]]
    z1 â‡ [[-0.00616586,  0.0020626 ,  0.0034962 ]
          [-0.05229879,  0.02726335, -0.02646869]
          [-0.02009991,  0.00368692,  0.02884556]
          [ 0.02153007, -0.01385322,  0.02600471]]
    z2 â‡ [[ 0.00092281, -0.00056678,  0.00095853]]
  }
  x_assess â† [[ 1.62434536, -0.61175641, -0.52817175]
              [-1.07296862,  0.86540763, -2.3015387 ]]
  y_assess â† [[1, 0, 1]]
  parametersâ€¿cacheâ€¿x_assessâ€¿y_assess
}

parametersâ€¿cacheâ€¿x_assessâ€¿y_assess â†© BackwardPropagationTestCase @

grads â† BackwardPropagation parametersâ€¿cacheâ€¿x_assessâ€¿y_assess

â€¢Out "dw1: "âˆ¾â€¢Repr grads.dw1
â€¢Out "db1: "âˆ¾â€¢Repr grads.db1
â€¢Out "dw2: "âˆ¾â€¢Repr grads.dw2
â€¢Out "db2: "âˆ¾â€¢Repr grads.db2

UpdateParameters â† {
  âŸ¨w1,b1,w2,b2âŸ©â€¿âŸ¨dw1,db1,dw2,db2âŸ©â€¿learning_rate:
  #â€¢Out "w1:"âˆ¾â€¢Repr â‰¢w1
  #â€¢Out "b1:"âˆ¾â€¢Repr â‰¢b1
  #â€¢Out "w2:"âˆ¾â€¢Repr â‰¢w2
  #â€¢Out "b2:"âˆ¾â€¢Repr â‰¢b2
  #â€¢Out "dw1:"âˆ¾â€¢Repr â‰¢dw1
  #â€¢Out "db1:"âˆ¾â€¢Repr â‰¢db1
  #â€¢Out "dw2:"âˆ¾â€¢Repr â‰¢dw2
  #â€¢Out "db2:"âˆ¾â€¢Repr â‰¢db2
  w1 â†© w1 - learning_rate Ã— dw1
  b1 â†© b1 - learning_rate Ã— db1
  w2 â†© w2 - learning_rate Ã— dw2
  b2 â†© b2 - learning_rate Ã— db2
  {
    w1 â‡ w1
    b1 â‡ b1
    w2 â‡ w2
    b2 â‡ b2
  }
}

UpdateParametersTestCase â† {
  Â·:
  parameters â† {
    w1 â‡ [[-0.00643025,  0.01936718]
          [-0.02410458,  0.03978052]
          [-0.01653973, -0.02096177]
          [ 0.01046864, -0.05990141]]
    b1 â‡ [[-1.02420756eÂ¯06]
          [ 1.27373948eÂ¯05]
          [ 8.32996807eÂ¯07]
          [-3.20136836eÂ¯06]]
    w2 â‡ [[-0.01041081, -0.04463285,  0.01758031,  0.04747113]]
    b2 â‡ [[0.00010457]]
  }
  grads â† {
    dw1 â‡ [[ 0.00023322, -0.00205423]
           [ 0.00082222, -0.00700776]
           [-0.00031831,  0.0028636 ]
           [-0.00092857,  0.00809933]]
    dw2 â‡ [[-1.75740039eÂ¯05,  3.70231337eÂ¯03, -1.25683095eÂ¯03, -2.55715317eÂ¯03]]
    db1 â‡ [[ 1.05570087eÂ¯07]
           [-3.81814487eÂ¯06]
           [-1.90155145eÂ¯07]
           [ 5.46467802eÂ¯07]]
    db2 â‡ [[-1.0892314eÂ¯05]]
  }
  parametersâ€¿grads
}

parametersâ€¿grads â†© UpdateParametersTestCase @
#â€¢Show parameters
#â€¢Show grads

parameters â†© UpdateParameters parametersâ€¿gradsâ€¿1.2

â€¢Out "w1: "âˆ¾â€¢Repr parameters.w1
â€¢Out "b1: "âˆ¾â€¢Repr parameters.b1
â€¢Out "w2: "âˆ¾â€¢Repr parameters.w2
â€¢Out "b2: "âˆ¾â€¢Repr parameters.b2

NNModel â† {
  xâ€¿yâ€¿n_hâ€¿learning_rateâ€¿num_iterationsâ€¿print_cost:
  n_xâ€¿Â·â€¿n_y â† LayerSizes xâ€¿y
  cacheâ€¿costâ€¿grads â† @â€¿@â€¿@
  parameters â† InitializeParameters n_xâ€¿n_hâ€¿n_y
  i â† 0
  {ğ•¤
    cache â†© ForwardPropagation xâ€¿parameters
    cost â†© ComputeCost cache.a2â€¿yâ€¿parameters
    grads â†© BackwardPropagation parametersâ€¿cacheâ€¿xâ€¿y
    parameters â†© UpdateParameters parametersâ€¿gradsâ€¿learning_rate
    {ğ•¤
      â€¢Out "Costs after iteration "âˆ¾(â€¢Repr i)âˆ¾": "âˆ¾â€¢Repr cost
    }âŸ(print_cost âˆ§ 0=1000|i) @
    i +â†© 1
  }âŸnum_iterations @
  parameters
}

NNModelTestCase â† {ğ•¤
  x_assess â† [[ 1.62434536, -0.61175641, -0.52817175]
              [-1.07296862,  0.86540763, -2.3015387 ]]
  y_assess â† [[1, 0, 1]]
  x_assessâ€¿y_assess
}

x_assessâ€¿y_assess â†© NNModelTestCase @

â€¢Out "x_assess = "
â€¢Show x_assess
â€¢Out "y_assess = "
â€¢Show y_assess

parameters â†© NNModel x_assessâ€¿y_assessâ€¿4â€¿1.02â€¿10000â€¿1

â€¢Out "w1 = "
â€¢Show parameters.w1
â€¢Out "b1 = "
â€¢Show parameters.b1
â€¢Out "w2 = "
â€¢Show parameters.w2
â€¢Out "b2 = "
â€¢Show parameters.b2

Predict â† {
  parametersâ€¿x:
  âŸ¨a2âŸ© â† ForwardPropagation xâ€¿parameters
  predictions â† a2 > 0.5
  #predictions
}

PredictTestCase â† {ğ•¤
  parameters â† {
    w1 â‡ [[-0.00615039,  0.0169021 ]
          [-0.02311792,  0.03137121]
          [-0.0169217 , -0.01752545]
          [ 0.00935436, -0.05018221]]
    b1 â‡ [[-8.97523455eÂ¯07]
          [ 8.15562092eÂ¯06]
          [ 6.04810633eÂ¯07]
          [-2.54560700eÂ¯06]]
    w2 â‡ [[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]
    b2 â‡ [[9.14954378eÂ¯05]]
  }
  x_assess â† [[ 1.62434536, -0.61175641, -0.52817175]
              [-1.07296862,  0.86540763, -2.3015387 ]]
  parametersâ€¿x_assess
}

parametersâ€¿x_assess â†© PredictTestCase @

predictions â† Predict parametersâ€¿x_assess

â€¢Out "predictions:"
â€¢Show predictions

â€¢Out "x:"
â€¢Show â‰¢x
â€¢Out "y:"
â€¢Show â‰¢y

parameters â†© NNModel xâ€¿yâ€¿4â€¿1.2â€¿10000â€¿1
predictions â†© Predict parametersâ€¿x
â€¢Out "Accuracy: "âˆ¾â€¢Repr 100Ã— (1âŠ‘â‰¢y)Ã·Ëœ (y Dot â‰predictions) + (Â¬y) Dot â‰Â¬predictions
