path_libreaddata â† "/Users/jra/prj/coursera/deeplearning/c1-NeuralNetworksAndDeepLearning/w2/02-LogisticRegressionWithANeuralNetworkMindset/libreaddata.dylib"
readTrainX â† path_libreaddata â€¢FFI "a"â€¿"read_train_x"
readTrainY â† path_libreaddata â€¢FFI "a"â€¿"read_train_y"
readTestX â† path_libreaddata â€¢FFI "a"â€¿"read_test_x"
readTestY â† path_libreaddata â€¢FFI "a"â€¿"read_test_y"
readTestListClasses â† path_libreaddata â€¢FFI "a"â€¿"read_test_list_classes"

train_set_x_orig â† @ -Ëœ ReadTrainX âŸ¨âŸ©
train_set_y â† ReadTrainY âŸ¨âŸ©
test_set_x_orig â† @ -Ëœ ReadTestX âŸ¨âŸ©
test_set_y â† ReadTestY âŸ¨âŸ©
classes â† ReadTestListClasses âŸ¨âŸ©

index â† 10
â€¢Out "y = "âˆ¾(â€¢Repr indexâŠ‘train_set_y)âˆ¾", it's a '"âˆ¾((indexâŠ‘train_set_y)âŠ‘classes)âˆ¾"' picture."
# ppm ex.
# 4 8  4 columns, 8 rows
# 255  max value
# data read in rows
#header_ppm â† "P6
#64 64
#255
#"
#image_ppm â† @ +Â¨ â¥Š index âŠ train_set_x_orig
#bytes_ppm â† header_ppm âˆ¾ image_ppm
#"z.ppm" â€¢file.Bytes bytes_ppm

m_train â† â‰ train_set_x_orig
m_test â† â‰ test_set_x_orig
num_px â† 1âŠ‘â‰¢train_set_x_orig

â€¢Out "Number of training examples: m_train = "âˆ¾â€¢Repr m_train
â€¢Out "Number of testing examples: m_test = "âˆ¾â€¢Repr m_test
â€¢Out "Height/Width of each image: num_px = "âˆ¾â€¢Repr num_px
â€¢Out "Each image is of size: "âˆ¾â€¢Repr â‰¢âŠtrain_set_x_orig
â€¢Out "train_set_x shape: "âˆ¾â€¢Repr â‰¢train_set_x_orig
â€¢Out "train_set_y shape: "âˆ¾â€¢Repr â‰¢train_set_y
â€¢Out "test_set_x shape: "âˆ¾â€¢Repr â‰¢test_set_x_orig
â€¢Out "test_set_y shape: "âˆ¾â€¢Repr â‰¢test_set_y

train_set_x_flatten â† â‰ (â‰ train_set_x_orig)â€¿âˆ˜ â¥Š train_set_x_orig
test_set_x_flatten â† â‰ (â‰ test_set_x_orig)â€¿âˆ˜ â¥Š test_set_x_orig

â€¢Out "train_set_x_flatten shape: "âˆ¾â€¢Repr â‰¢train_set_x_flatten
â€¢Out "train_set_y shape: "âˆ¾â€¢Repr â‰¢train_set_y
â€¢Out "test_set_x_flatten shape: "âˆ¾â€¢Repr â‰¢test_set_x_flatten
â€¢Out "test_set_y shape: "âˆ¾â€¢Repr â‰¢test_set_y
â€¢Out "sanity check after reshpaing: "âˆ¾â€¢Repr 5â†‘âŠâ‰1 train_set_x_flatten

train_set_x â† train_set_x_flatten Ã· 255
test_set_x â† test_set_x_flatten Ã· 255

Sigmoid â† { Ã·1+â‹†-ğ•© }
#Sigmoid â† Ã·âˆ˜(1âŠ¸+)âˆ˜â‹†âˆ˜-

â€¢Out "Sigmoid "âˆ¾(â€¢Repr 0â€¿2)âˆ¾" = "âˆ¾â€¢Repr Sigmoid 0â€¿2

InitializeWithZeros â† { (ğ•©â€¿1â¥Š0)â€¿0 }

tdim â† 2
twâ€¿tb â† InitializeWithZeros tdim

â€¢Out "tw = "âˆ¾â€¢Repr tw
â€¢Out "tb = "âˆ¾â€¢Repr tb

#Dot â† { ğ•¨ +Ëâˆ˜Ã—â‰1â€¿âˆ ğ•© }
#Dot â† +Ëâˆ˜Ã—â‰1â€¿âˆ
#Dot â† {
#  1â‰¡âŠ‘â‰ ğ•¨? â‰+Â´ (â¥Šğ•¨) Ã— <Ë˜ ğ•©;
#  1â‰¡1âŠ‘â‰¢ğ•©? (â‰ ğ•¨)â€¿1â¥Š (+Â´(â¥Šğ•©)âŠ¸Ã—)Ë˜ğ•¨;
#  ğ•¨ +Ëâˆ˜Ã—â‰1â€¿âˆ ğ•©
#}

cblasRowMajorâ†101
cblasColMajorâ†102

cblasNoTransâ†111
cblasTransâ†112
cblasConjTransâ†113
cblasConjNoTransâ†114
cblasTransOptionsâ†âŸ¨
  cblasNoTrans
  cblasTrans
âŸ©

#path_blas_lib â† "/Users/jra/.local/share/virtualenvs/deeplearning-TAU9TGrL/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib"  # no cblas
#path_blas_lib â† "/usr/local/lib/libgslcblas.dylib"  # gsl (not parallelized)
#path_blas_lib â† "/usr/local/Cellar/openblas/0.3.20/lib/libopenblasp-r0.3.20.dylib"  # brew package (good)
path_blas_lib â† "/Users/jra/pks/openblas/lib/libopenblas_haswellp-r0.3.17.dylib"  # compiled locally (best)

p â† (1-Ëœ+`Ã—Â¬)âˆ˜=âŸœâŠâŠ¸âŠ” " & cblas_dgemm u32 u32 u32 i32 i32 i32 f64 *f64 i32 *f64 i32 f64 &f64 i32"
gemm â† path_blas_lib â€¢FFI p
Cblas_gemm_ab â† {
  aâ€¿b:
  mâ€¿kâ†â‰¢a â‹„ kxâ€¿nâ†â‰¢b â‹„ !kâ‰¡kx
  câ†mâ€¿nâ¥Š0
  Gemm 101â€¿111â€¿111â€¿mâ€¿nâ€¿kâ€¿1â€¿aâ€¿kâ€¿bâ€¿nâ€¿0â€¿câ€¿n
}

#                                   lay tA  m   n   Î±   A    lda x    incx Î²  y    incy
p â†© (1-Ëœ+`Ã—Â¬)âˆ˜=âŸœâŠâŠ¸âŠ” " & cblas_dgemv u32 u32 i32 i32 f64 *f64 i32 *f64 i32 f64 &f64 i32"
gemv â† path_blas_lib â€¢FFI p
Cblas_gemv_axpby â† {
  transâ€¿aâ€¿xâ€¿betaâ€¿y:
  ctrans â† transâŠ‘cblasTransOptions  # 0-no trans, 1-trans
  mâ€¿nâ†â‰¢a â‹„ nxâ†âŠ‘â‰¢x â‹„ nyâ†âŠ‘â‰¢y
  !nx=transâŠ‘nâ€¿m â‹„ !ny=transâŠ‘mâ€¿n
  incx â† 1
  incy â† 1
  alpha â† 1
  Gemv cblasRowMajorâ€¿ctransâ€¿mâ€¿nâ€¿alphaâ€¿aâ€¿nâ€¿xâ€¿incxâ€¿betaâ€¿yâ€¿incy
}

Cblas_gemv_ax â† {
  transâ€¿aâ€¿x:
  ctrans â† transâŠ‘cblasTransOptions  # 0-no trans, 1-trans
  mâ€¿nâ†â‰¢a
  y â† (transâŠ‘mâ€¿n)â€¿1â¥Š0
  nxâ†âŠ‘â‰¢x â‹„ nyâ†âŠ‘â‰¢y
  !nx=transâŠ‘nâ€¿m â‹„ !ny=transâŠ‘mâ€¿n
  incx â† 1
  incy â† 1
  alpha â† 1
  beta â† 0
  Gemv cblasRowMajorâ€¿ctransâ€¿mâ€¿nâ€¿alphaâ€¿aâ€¿nâ€¿xâ€¿incxâ€¿betaâ€¿yâ€¿incy
}

Dot â† {
  1â‰¡âŠ‘â‰ ğ•¨? â‰ Cblas_gemv_ax 1â€¿ğ•©â€¿(â‰ğ•¨);
  1â‰¡1âŠ‘â‰¢ğ•©? Cblas_gemv_ax 0â€¿ğ•¨â€¿ğ•©;
  Cblas_gemm_ab ğ•¨â€¿ğ•©
}

Propagate â† {
  wâ€¿bâ€¿xâ€¿y:
  m â† 1âŠ‘â‰¢y
  n â† âŠ‘â‰¢w
  a â† Sigmoid b + (â‰w) Dot x
  #a â† Sigmoid 1â€¿m â¥Š Cblas_gemv_axpby 1â€¿xâ€¿wâ€¿1â€¿(mâ€¿1â¥Šb)
  #a â† Sigmoid b + 1â€¿m â¥Š Cblas_gemv_ax 1â€¿xâ€¿w
  cost â† âŠ‘-Ã·âŸœm+ËË˜(yÃ—â‹†â¼a)+(Â¬y)Ã—â‹†â¼Â¬a
  #cost â† âŠ‘-Ã·âŸœm+Ëâ‰(yÃ—â‹†â¼a)+(Â¬y)Ã—â‹†â¼Â¬a
  dw â† Ã·âŸœm x Dot â‰a-y
  #dw â† Ã·âŸœm Cblas_gemv_axbpy 0â€¿xâ€¿(â‰a-y)â€¿0â€¿(nâ€¿1â¥Š0)
  #dw â† Ã·âŸœm Cblas_gemv_ax 0â€¿xâ€¿(â‰a-y)
  db â† âŠ‘ Ã·âŸœm +ËË˜a-y
  #db â† âŠ‘ Ã·âŸœm +Ëâ‰a-y
  âŸ¨dwâ€¿db, costâŸ©
}

wâ€¿bâ€¿xâ€¿y â† âŸ¨âˆ˜â€¿1â¥Š1â€¿2, 2, >âŸ¨1â€¿2â€¿Â¯1, 3â€¿4â€¿Â¯3.2âŸ©, 1â€¿âˆ˜â¥Š1â€¿0â€¿1âŸ©
gradsâ€¿cost â† Propagate wâ€¿bâ€¿xâ€¿y

â€¢Out "Propagate test:"
â€¢Out "dw = "âˆ¾â€¢Repr âŠ‘grads
â€¢Out "db = "âˆ¾â€¢Repr 1âŠ‘grads
â€¢Out "cost = "âˆ¾â€¢Repr cost

Optimize â† {
  wâ€¿bâ€¿xâ€¿yâ€¿dwâ€¿dbâ€¿costsâ€¿0â€¿learning_rateâ€¿print_cost:
    âŸ¨w, bâŸ©â€¿âŸ¨dw, dbâŸ©â€¿costs;
  wâ€¿bâ€¿xâ€¿yâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost:
    #â€¢Out num_iterations
    gradsâ€¿cost â† Propagate wâ€¿bâ€¿xâ€¿y
    dwâ€¿db â†© grads
    w -â†© learning_rateÃ—dw
    b -â†© learning_rateÃ—db
    {ğ•¤,costs âˆ¾â†© ğ•©}âŸ(0=100|num_iterations) cost
    {ğ•¤,â€¢Out â€¢Fmt ğ•©}âŸ(print_cost âˆ§ 0=100|num_iterations) cost
    num_iterations -â†© 1
    Optimize wâ€¿bâ€¿xâ€¿yâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost
}

Optimize2 â† {
  wâ€¿bâ€¿xâ€¿yâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost:
  gradsâ€¿cost â† @â€¿@
  i â† 0
  {
    ğ•¤
    {
      gradsâ€¿cost â†© Propagate wâ€¿bâ€¿xâ€¿y
      dwâ€¿db â†© grads
      w -â†© learning_rateÃ—dw
      b -â†© learning_rateÃ—db
      {ğ•¤,costs âˆ¾â†© ğ•©}âŸ(0=100|i) cost
      {ğ•¤,â€¢Out â€¢Fmt ğ•©}âŸ(print_cost âˆ§ 0=100|i) cost
      i +â†© 1
    }
  }âŸnum_iterations @
  âŸ¨w, bâŸ©â€¿âŸ¨dw, dbâŸ©â€¿costs
}

dwâ€¿dbâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost â† @â€¿@â€¿@â€¿@â€¿@
params â† @
costs â† âŸ¨âŸ©
wâ€¿bâ€¿xâ€¿yâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost â†© âŸ¨w, b, x, y, 0, 0, âŸ¨âŸ©, 100, 0.009, 0âŸ©
paramsâ€¿gradsâ€¿costs â†© Optimize wâ€¿bâ€¿xâ€¿yâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost

â€¢Out "Optimize test:"
â€¢Out "w = "âˆ¾â€¢Repr âŠparams
â€¢Out "b = "âˆ¾â€¢Repr 1âŠparams
â€¢Out "dw = "âˆ¾â€¢Repr âŠ‘grads
â€¢Out "db = "âˆ¾â€¢Repr 1âŠ‘grads

Predict â† {
  wâ€¿bâ€¿x:
  m â† 1âŠ‘â‰¢x
  y_prediction â† 1â€¿mâ¥Š0
  w â†© (âŠ‘â‰¢x)â€¿1â¥Šw
  a â† Sigmoid b + (â‰w) Dot x
  y_prediction â†© a>0.5
  y_prediction
}

wâ€¿bâ€¿x â†© âŸ¨âˆ˜â€¿1â¥Š0.1124579â€¿0.23106775, Â¯0.3, >âŸ¨1.0â€¿Â¯1.1â€¿Â¯3.2, 1.2â€¿2.0â€¿0.1âŸ©âŸ©
â€¢Out "predictions = "âˆ¾â€¢Repr Predict wâ€¿bâ€¿x

Model â† {
  x_trainâ€¿y_trainâ€¿x_testâ€¿y_testâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost:
  wâ€¿b â† InitializeWithZeros âŠ‘â‰¢x_train
  dwâ€¿dbâ€¿costs â† @â€¿@â€¿âŸ¨âŸ©
  parametersâ€¿grads â† @â€¿@
  parametersâ€¿gradsâ€¿costs â†© Optimize2 wâ€¿bâ€¿x_trainâ€¿y_trainâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost
  #parametersâ€¿gradsâ€¿costs â†© Optimize wâ€¿bâ€¿x_trainâ€¿y_trainâ€¿dwâ€¿dbâ€¿costsâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost

  # Retrieve parameters w and b from dictionary "parameters"
  wâ€¿b â†© parameters

  # Predict test/train set examples (â‰ˆ 2 lines of code)
  y_prediction_test â† Predict wâ€¿bâ€¿x_test
  y_prediction_train â† Predict wâ€¿bâ€¿x_train
  ### END CODE HERE ###

  # Print train/test Errors
  â€¢Out "train accuracy: "âˆ¾(â€¢Repr âŠ‘100-100Ã—(+Â´Ã·â‰ )Ë˜|y_prediction_train-y_train)âˆ¾"%"
  â€¢Out "test accuracy: "âˆ¾(â€¢Repr âŠ‘100-100Ã—(+Â´Ã·â‰ )Ë˜|y_prediction_test-y_test)âˆ¾"%"

  âŸ¨
    costs
    y_prediction_test
    y_prediction_train
    w
    b
    learning_rate
    num_iterations
  âŸ©
}

num_iterationsâ€¿learning_rateâ€¿print_cost â†© 2000â€¿0.005â€¿0
#num_iterationsâ€¿learning_rateâ€¿print_cost â†© 40â€¿0.005â€¿0
train_set_y â†© 1â€¿âˆ˜â¥Štrain_set_y
test_set_y â†© 1â€¿âˆ˜â¥Štest_set_y
d â† Model train_set_xâ€¿train_set_yâ€¿test_set_xâ€¿test_set_yâ€¿num_iterationsâ€¿learning_rateâ€¿print_cost
â€¢Out "b "âˆ¾â€¢Fmt 4âŠ‘d
â€¢Out "costs "âˆ¾â€¢Fmt âŠ‘d
#â€¢Out "costs "âˆ¾â€¢Fmt Â¯1âŠ‘âŠ‘d
